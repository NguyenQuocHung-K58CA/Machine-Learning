{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.special import expit\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    def __init__(self, n_output, n_features, n_hidden=30, \n",
    "                l1 = 0.0, l2 = 0.0, epochs = 500, eta = 0.001,\n",
    "                alpha = 0.0, decrease_const = 0.0, shuffle = True,\n",
    "                minibatches = 1, random_state = None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "    \n",
    "    def _encode_labels(self, y, k):\n",
    "        one_hot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            one_hot[val, idx] = 1.0\n",
    "        return one_hot\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        w1 = np.random.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features+1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0, size = self.n_output*(self.n_hidden+1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        return w1, w2\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return expit(z)\n",
    "    \n",
    "    def _sigmoid_gradient(self, z):\n",
    "        sg = self._sigmoid(z)\n",
    "        return sg*(1-sg)\n",
    "    \n",
    "    def _add_bias_unit(self, X, how = 'column'):\n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('how must be row or column')\n",
    "        \n",
    "        return X_new\n",
    "    \n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "    \n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) + np.sum(w2[:, 1:] ** 2))\n",
    "    \n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() + np.abs(w2[:, 1:]).sum())\n",
    "    \n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1 - y_enc) * np.log(1 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        \n",
    "        return cost\n",
    "        \n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        #backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "        \n",
    "        # regularize\n",
    "        grad1[:, 1:] += (w1[:, 1:] * (self.l1 + self.l2))\n",
    "        grad2[:, 1:] += (w2[:, 1:] * (self.l1 + self.l2))\n",
    "        \n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis = 0)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X, y, print_progress = False):\n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "        \n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            #adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const * i)\n",
    "            \n",
    "            if print_progress:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' %(i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "            \n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_data = X_data[idx], y_data[idx]\n",
    "                \n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            \n",
    "            for idx in mini:\n",
    "                \n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X[idx], self.w1, self.w2)\n",
    "                cost = self._get_cost(y_enc = y_enc[:, idx],\n",
    "                                      output = a3,\n",
    "                                      w1 = self.w1, \n",
    "                                      w2 = self.w2 )\n",
    "                self.cost_.append(cost)\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                a3=a3, z2=z2,\n",
    "                y_enc=y_enc[:, idx],\n",
    "                w1=self.w1,\n",
    "                w2=self.w2)\n",
    "                # update weights\n",
    "                delta_w1, delta_w2 = self.eta * grad1,\\\n",
    "                self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "                \n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
